{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro To Full Stack LLM Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Goals\n",
    "Our goal is for you to understand:\n",
    "* How and why Iron Python notebooks are used for prototyping\n",
    "* Each section of code\n",
    "\n",
    "Overall, we hope you will feel enabled to do more advanced tutorials with standard data science tools in the future.\n",
    "\n",
    "## If You Get Stuck\n",
    "Just get someone's attention for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Run A Shell Command\n",
    "Lines of code that start with a `%` in an Iron Python notebook execute a `magic command`. In the next cell, the magic command executes a bash command to show you what system you are running on.\n",
    "\n",
    "Run the code cell below by selecting it and using one of the following methods:\n",
    "* `shift + enter`: run and move to next\n",
    "* `ctrl + enter`: run\n",
    "* Press the run button in the toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.2 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "lsb_release -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Install Libraries\n",
    "### Context\n",
    "Typically when you open a Juypter notebook file for the first time on JupyterHub, you will need to install the third-party libraries you will use to develop your software.\n",
    "\n",
    "We are installing the following libraries:\n",
    "- `huggingface_hub`: the API we will use to access a hosted version of Falcon LLM\n",
    "- `dotenv`: allows you to securely store your Hugging Face token\n",
    "- `langchain`: a popular library for making LLMs easier to use\n",
    "\n",
    "### How To\n",
    "1. Option 1: Click the cell and press `shift + enter` to execute and move to next cell\n",
    "2. Option 2: Click the run button in the jupyterhub toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /home/jupyter-toby/.local/lib/python3.9/site-packages (0.0.260)\n",
      "Requirement already satisfied: huggingface_hub in /home/jupyter-toby/.local/lib/python3.9/site-packages (0.16.4)\n",
      "Requirement already satisfied: python-dotenv in /home/jupyter-toby/.local/lib/python3.9/site-packages (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/tljh/user/lib/python3.9/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/tljh/user/lib/python3.9/site-packages (from langchain) (1.4.49)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (2.8.5)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (0.0.20)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/tljh/user/lib/python3.9/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface_hub) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: filelock in /home/jupyter-toby/.local/lib/python3.9/site-packages (from huggingface_hub) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/jupyter-toby/.local/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jupyter-toby/.local/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain huggingface_hub python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Get Hugging Face Access Token\n",
    "### Context\n",
    "To use an API, you typically need to generate a \"password\" for your code to use to login to your account. This is typically called an \"Access Token\".\n",
    "\n",
    "To store this password securely, it's traditional to use a `.env` file so that the password doesn't accidentally get committed to a Git reposititory. So, we will run a `bash` command below to create this `.env` file.\n",
    "\n",
    "### Instructions\n",
    "1. Create an account at https://huggingface.co\n",
    "2. Go to https://huggingface.co/settings/token\n",
    "3. Click on \"New Token\" button\n",
    "4. For **Name** put intro-to-full-stack-llm-token. For **Role** put Read.\n",
    "5. Click generate token\n",
    "6. Copy the token to your clipboard\n",
    "7. Paste the token below, replacing `your_hugging_face_token`\n",
    "8. Run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUGGINGFACE_API_TOKEN=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\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"HUGGINGFACE_API_TOKEN=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\" > .env\n",
    "cat .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Load Hugging Face Token Into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token=os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Setup Hosted LLM\n",
    "### Context\n",
    "The model we will use today is [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
    "\n",
    "**Falcon** is what TII of the UAE government named this model, fittingly because they like falcons. For a while, the Falcon model was the highest performing LLM on the [Hugging Face LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). It has since been surpassed by Facebook's Llama 2 model and others.\n",
    "\n",
    "**7b** refers to the number of parameters that the model has. There is a more resource-intensive but powerful model called `falcon-40b` that has 40b parameters.\n",
    "\n",
    "**instruct** refers to the fact that the LLM is \"instruction fine-tuned,\" which means that the model has been specially fine-tuned to have the UX of a human assistant. Non-instruction fine-tuned LLMs are much more difficult to use.\n",
    "\n",
    "### Instructions\n",
    "Run the code below.\n",
    "\n",
    "Read the comments explaining what each line does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'tiiuae/falcon-7b-instruct', 'task': None, 'model_kwargs': {'temperature': 0.7, 'max_new_tokens': 200}}\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub  # Allows us to use LLMs from Hugging Face Hub\n",
    "\n",
    "# We set up an LLM for use in the next task\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=token, # Your \"password\"\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7, # Adjusts how \"random\" or \"deterministic\" the model will output\n",
    "        \"max_new_tokens\":200 # Maximum number of tokens the model will output\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Prompt Falcon-7B\n",
    "### Context\n",
    "Now you can prompt Falcon-7B from the Python interpreter similar to how you prompt ChatGPT from its GUI interface.\n",
    "\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Understand the comments explaining the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Gather all the necessary ingredients. You’ll need pasta, water, salt and a pot or pan.\n",
      "2. Place a pot or pan on the stove and heat the water. Add the salt to the water and stir.\n",
      "3. Once the water boils, add the pasta to the pot and stir to prevent sticking.\n",
      "4. Cook the pasta according to the package instructions. You’ll want to cook it for about 8-10 minutes.\n",
      "5. Once the pasta is done cooking, drain it in a colander and place it back in the pot or pan.\n",
      "6. Add your desired sauce and stir the pasta to combine.\n",
      "7. Serve your pasta and enjoy!\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate is a feature of LangChain for defining reusable prompts\n",
    "#\n",
    "# LLMChain is called \"chain\" because LangChain started as a \n",
    "# library for \"chaining together\" multiple successive LLM calls\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables= [\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"\"\"\n",
    "User: Give me step by step instructions to cook pasta\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Make Another Prompt\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Modify the code to make any other queries\n",
    "3. Compare the performance against ChatGPT in a separate window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>The capitol of British Columbia is Victoria.</b>\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "User: What is the capitol of British Columbia?\n",
    "\"\"\"\n",
    "\n",
    "completion = llm_chain.run(question)\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
